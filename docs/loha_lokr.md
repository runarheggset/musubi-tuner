> ğŸ“ Click on the language section to expand / è¨€èªã‚’ã‚¯ãƒªãƒƒã‚¯ã—ã¦å±•é–‹

# LoHa / LoKr (LyCORIS)

## Overview / æ¦‚è¦

In addition to standard LoRA, Musubi Tuner supports **LoHa** (Low-rank Hadamard Product) and **LoKr** (Low-rank Kronecker Product) as alternative parameter-efficient fine-tuning methods. These are based on techniques from the [LyCORIS](https://github.com/KohakuBlueleaf/LyCORIS) project.

- **LoHa**: Represents weight updates as a Hadamard (element-wise) product of two low-rank matrices. Reference: [FedPara (arXiv:2108.06098)](https://arxiv.org/abs/2108.06098)
- **LoKr**: Represents weight updates as a Kronecker product with optional low-rank decomposition. Reference: [LoKr (arXiv:2309.14859)](https://arxiv.org/abs/2309.14859)

The algorithms and recommended settings are described in the [LyCORIS documentation](https://github.com/KohakuBlueleaf/LyCORIS/blob/main/docs/Algo-List.md) and [guidelines](https://github.com/KohakuBlueleaf/LyCORIS/blob/main/docs/Guidelines.md).

Both methods target Linear layers only (Conv2d layers are not supported in this implementation).

This feature is experimental.

<details>
<summary>æ—¥æœ¬èª</summary>

Musubi Tunerã§ã¯ã€æ¨™æº–çš„ãªLoRAã«åŠ ãˆã€ä»£æ›¿ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŠ¹ç‡ã®è‰¯ã„ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ‰‹æ³•ã¨ã—ã¦ **LoHa**ï¼ˆLow-rank Hadamard Productï¼‰ã¨ **LoKr**ï¼ˆLow-rank Kronecker Productï¼‰ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã¾ã™ã€‚ã“ã‚Œã‚‰ã¯ [LyCORIS](https://github.com/KohakuBlueleaf/LyCORIS) ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®æ‰‹æ³•ã«åŸºã¥ã„ã¦ã„ã¾ã™ã€‚

- **LoHa**: é‡ã¿ã®æ›´æ–°ã‚’2ã¤ã®ä½ãƒ©ãƒ³ã‚¯è¡Œåˆ—ã®Hadamardç©ï¼ˆè¦ç´ ã”ã¨ã®ç©ï¼‰ã§è¡¨ç¾ã—ã¾ã™ã€‚å‚è€ƒæ–‡çŒ®: [FedPara (arXiv:2108.06098)](https://arxiv.org/abs/2108.06098)
- **LoKr**: é‡ã¿ã®æ›´æ–°ã‚’Kroneckerç©ã¨ã€ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã®ä½ãƒ©ãƒ³ã‚¯åˆ†è§£ã§è¡¨ç¾ã—ã¾ã™ã€‚å‚è€ƒæ–‡çŒ®: [LoKr (arXiv:2309.14859)](https://arxiv.org/abs/2309.14859)

ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã¨æ¨å¥¨è¨­å®šã¯[LyCORISã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ è§£èª¬](https://github.com/KohakuBlueleaf/LyCORIS/blob/main/docs/Algo-List.md)ã¨[ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³](https://github.com/KohakuBlueleaf/LyCORIS/blob/main/docs/Guidelines.md)ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚

ã„ãšã‚Œã‚‚Linearå±¤ã®ã¿ã‚’å¯¾è±¡ã¨ã—ã¦ã„ã¾ã™ï¼ˆConv2då±¤ã¯ã“ã®å®Ÿè£…ã§ã¯ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã¾ã›ã‚“ï¼‰ã€‚

ã“ã®æ©Ÿèƒ½ã¯å®Ÿé¨“çš„ãªã‚‚ã®ã§ã™ã€‚

</details>

## Acknowledgments / è¬è¾

The LoHa and LoKr implementations in Musubi Tuner are based on the [LyCORIS](https://github.com/KohakuBlueleaf/LyCORIS) project by [KohakuBlueleaf](https://github.com/KohakuBlueleaf). We would like to express our sincere gratitude for the excellent research and open-source contributions that made this implementation possible.

<details>
<summary>æ—¥æœ¬èª</summary>

Musubi Tunerã®LoHaãŠã‚ˆã³LoKrã®å®Ÿè£…ã¯ã€[KohakuBlueleaf](https://github.com/KohakuBlueleaf)æ°ã«ã‚ˆã‚‹[LyCORIS](https://github.com/KohakuBlueleaf/LyCORIS)ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã«åŸºã¥ã„ã¦ã„ã¾ã™ã€‚ã“ã®å®Ÿè£…ã‚’å¯èƒ½ã«ã—ã¦ãã ã•ã£ãŸç´ æ™´ã‚‰ã—ã„ç ”ç©¶ã¨ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ã¸ã®è²¢çŒ®ã«å¿ƒã‹ã‚‰æ„Ÿè¬ã„ãŸã—ã¾ã™ã€‚

</details>

## Supported architectures / å¯¾å¿œã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£

LoHa and LoKr automatically detect the model architecture and apply appropriate default settings. The following architectures are supported:

- HunyuanVideo
- HunyuanVideo 1.5
- Wan 2.1/2.2
- FramePack
- FLUX.1 Kontext / FLUX.2
- Qwen-Image series
- Z-Image

Kandinsky5 is **not supported** with LoHa/LoKr (it requires special handling that is incompatible with automatic architecture detection).

Each architecture has its own default `exclude_patterns` to skip non-trainable modules (e.g., modulation layers, normalization layers). These are applied automatically when using LoHa/LoKr.

<details>
<summary>æ—¥æœ¬èª</summary>

LoHaã¨LoKrã¯ã€ãƒ¢ãƒ‡ãƒ«ã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’è‡ªå‹•ã§æ¤œå‡ºã—ã€é©åˆ‡ãªãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã‚’é©ç”¨ã—ã¾ã™ã€‚ä»¥ä¸‹ã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã«å¯¾å¿œã—ã¦ã„ã¾ã™:

- HunyuanVideo
- HunyuanVideo 1.5
- Wan 2.1/2.2
- FramePack
- FLUX.1 Kontext / FLUX.2
- Qwen-Imageç³»
- Z-Image

Kandinsky5ã¯LoHa/LoKrã« **å¯¾å¿œã—ã¦ã„ã¾ã›ã‚“**ï¼ˆè‡ªå‹•ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£æ¤œå‡ºã¨äº’æ›æ€§ã®ãªã„ç‰¹æ®Šãªå‡¦ç†ãŒå¿…è¦ã§ã™ï¼‰ã€‚

å„ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã«ã¯ã€å­¦ç¿’å¯¾è±¡å¤–ã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ï¼ˆmodulationå±¤ã€normalizationå±¤ãªã©ï¼‰ã‚’ã‚¹ã‚­ãƒƒãƒ—ã™ã‚‹ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã® `exclude_patterns` ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã™ã€‚LoHa/LoKrä½¿ç”¨æ™‚ã«ã¯ã“ã‚Œã‚‰ãŒè‡ªå‹•çš„ã«é©ç”¨ã•ã‚Œã¾ã™ã€‚

</details>

## Training / å­¦ç¿’

To use LoHa or LoKr, change the `--network_module` argument in your training command. All other training options (dataset config, optimizer, etc.) remain the same as LoRA.

<details>
<summary>æ—¥æœ¬èª</summary>

LoHaã¾ãŸã¯LoKrã‚’ä½¿ç”¨ã™ã‚‹ã«ã¯ã€å­¦ç¿’ã‚³ãƒãƒ³ãƒ‰ã® `--network_module` å¼•æ•°ã‚’å¤‰æ›´ã—ã¾ã™ã€‚ãã®ä»–ã®å­¦ç¿’ã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼ˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆè¨­å®šã€ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãªã©ï¼‰ã¯LoRAã¨åŒã˜ã§ã™ã€‚

</details>

### LoHa

```bash
accelerate launch --num_cpu_threads_per_process 1 --mixed_precision bf16 src/musubi_tuner/hv_train_network.py \
    --dit path/to/dit \
    --dataset_config path/to/toml \
    --sdpa --mixed_precision bf16 --fp8_base \
    --optimizer_type adamw8bit --learning_rate 2e-4 --gradient_checkpointing \
    --network_module networks.loha --network_dim 32 --network_alpha 16 \
    --max_train_epochs 16 --save_every_n_epochs 1 \
    --output_dir path/to/output --output_name my-loha
```

### LoKr

```bash
accelerate launch --num_cpu_threads_per_process 1 --mixed_precision bf16 src/musubi_tuner/hv_train_network.py \
    --dit path/to/dit \
    --dataset_config path/to/toml \
    --sdpa --mixed_precision bf16 --fp8_base \
    --optimizer_type adamw8bit --learning_rate 2e-4 --gradient_checkpointing \
    --network_module networks.lokr --network_dim 32 --network_alpha 16 \
    --max_train_epochs 16 --save_every_n_epochs 1 \
    --output_dir path/to/output --output_name my-lokr
```

Replace `hv_train_network.py` with the appropriate training script for your architecture (e.g., `wan_train_network.py`, `fpack_train_network.py`, etc.).

<details>
<summary>æ—¥æœ¬èª</summary>

`hv_train_network.py` ã®éƒ¨åˆ†ã¯ã€ãŠä½¿ã„ã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã«å¯¾å¿œã™ã‚‹å­¦ç¿’ã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼ˆ`wan_train_network.py`, `fpack_train_network.py` ãªã©ï¼‰ã«ç½®ãæ›ãˆã¦ãã ã•ã„ã€‚

</details>

### Common training options / å…±é€šã®å­¦ç¿’ã‚ªãƒ—ã‚·ãƒ§ãƒ³

The following `--network_args` options are available for both LoHa and LoKr, same as LoRA:

| Option | Description |
|---|---|
| `verbose=True` | Display detailed information about the network modules |
| `rank_dropout=0.1` | Apply dropout to the rank dimension during training |
| `module_dropout=0.1` | Randomly skip entire modules during training |
| `exclude_patterns=[r'...']` | Exclude modules matching the regex patterns (in addition to architecture defaults) |
| `include_patterns=[r'...']` | Include only modules matching the regex patterns |

See [Advanced configuration](advanced_config.md) for details on how to specify `network_args`.

<details>
<summary>æ—¥æœ¬èª</summary>

ä»¥ä¸‹ã® `--network_args` ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã¯ã€LoRAã¨åŒæ§˜ã«LoHaã¨LoKrã®ä¸¡æ–¹ã§ä½¿ç”¨ã§ãã¾ã™:

| ã‚ªãƒ—ã‚·ãƒ§ãƒ³ | èª¬æ˜ |
|---|---|
| `verbose=True` | ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®è©³ç´°æƒ…å ±ã‚’è¡¨ç¤º |
| `rank_dropout=0.1` | å­¦ç¿’æ™‚ã«ãƒ©ãƒ³ã‚¯æ¬¡å…ƒã«ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆã‚’é©ç”¨ |
| `module_dropout=0.1` | å­¦ç¿’æ™‚ã«ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«å…¨ä½“ã‚’ãƒ©ãƒ³ãƒ€ãƒ ã«ã‚¹ã‚­ãƒƒãƒ— |
| `exclude_patterns=[r'...']` | æ­£è¦è¡¨ç¾ãƒ‘ã‚¿ãƒ¼ãƒ³ã«ä¸€è‡´ã™ã‚‹ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’é™¤å¤–ï¼ˆã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã«è¿½åŠ ï¼‰ |
| `include_patterns=[r'...']` | æ­£è¦è¡¨ç¾ãƒ‘ã‚¿ãƒ¼ãƒ³ã«ä¸€è‡´ã™ã‚‹ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã¿ã‚’å¯¾è±¡ã¨ã™ã‚‹ |

`network_args` ã®æŒ‡å®šæ–¹æ³•ã®è©³ç´°ã¯ [é«˜åº¦ãªè¨­å®š](advanced_config.md) ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚

</details>

### LoKr-specific option: `factor` / LoKrå›ºæœ‰ã®ã‚ªãƒ—ã‚·ãƒ§ãƒ³: `factor`

LoKr decomposes weight dimensions using factorization. The `factor` option controls how dimensions are split:

- `factor=-1` (default): Automatically find balanced factors. For example, dimension 512 is split into (16, 32).
- `factor=N` (positive integer): Force factorization using the specified value. For example, `factor=4` splits dimension 512 into (4, 128).

```bash
--network_args "factor=4"
```

When `network_dim` (rank) is large enough relative to the factorized dimensions, LoKr uses a full matrix instead of a low-rank decomposition for the second factor. A warning will be logged in this case.

<details>
<summary>æ—¥æœ¬èª</summary>

LoKrã¯é‡ã¿ã®æ¬¡å…ƒã‚’å› æ•°åˆ†è§£ã—ã¦åˆ†å‰²ã—ã¾ã™ã€‚`factor` ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã§ãã®åˆ†å‰²æ–¹æ³•ã‚’åˆ¶å¾¡ã—ã¾ã™:

- `factor=-1`ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰: ãƒãƒ©ãƒ³ã‚¹ã®è‰¯ã„å› æ•°ã‚’è‡ªå‹•çš„ã«è¦‹ã¤ã‘ã¾ã™ã€‚ä¾‹ãˆã°ã€æ¬¡å…ƒ512ã¯(16, 32)ã«åˆ†å‰²ã•ã‚Œã¾ã™ã€‚
- `factor=N`ï¼ˆæ­£ã®æ•´æ•°ï¼‰: æŒ‡å®šã—ãŸå€¤ã§å› æ•°åˆ†è§£ã—ã¾ã™ã€‚ä¾‹ãˆã°ã€`factor=4` ã¯æ¬¡å…ƒ512ã‚’(4, 128)ã«åˆ†å‰²ã—ã¾ã™ã€‚

```bash
--network_args "factor=4"
```

`network_dim`ï¼ˆãƒ©ãƒ³ã‚¯ï¼‰ãŒå› æ•°åˆ†è§£ã•ã‚ŒãŸæ¬¡å…ƒã«å¯¾ã—ã¦ååˆ†ã«å¤§ãã„å ´åˆã€LoKrã¯ç¬¬2å› å­ã«ä½ãƒ©ãƒ³ã‚¯åˆ†è§£ã§ã¯ãªããƒ•ãƒ«è¡Œåˆ—ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚ãã®å ´åˆã€è­¦å‘ŠãŒãƒ­ã‚°ã«å‡ºåŠ›ã•ã‚Œã¾ã™ã€‚

</details>

## How LoHa and LoKr work / LoHaã¨LoKrã®ä»•çµ„ã¿

### LoHa

LoHa represents the weight update as a Hadamard (element-wise) product of two low-rank matrices:

```
Î”W = (W1a Ã— W1b) âŠ™ (W2a Ã— W2b)
```

where `W1a`, `W1b`, `W2a`, `W2b` are low-rank matrices with rank `network_dim`. This means LoHa has roughly **twice the number of trainable parameters** compared to LoRA at the same rank, but can capture more complex weight structures due to the element-wise product.

### LoKr

LoKr represents the weight update using a Kronecker product:

```
Î”W = W1 âŠ— W2    (where W2 = W2a Ã— W2b in low-rank mode)
```

The original weight dimensions are factorized (e.g., a 512Ã—512 weight might be split so that W1 is 16Ã—16 and W2 is 32Ã—32). W1 is always a full matrix (small), while W2 can be either low-rank decomposed or a full matrix depending on the rank setting. LoKr tends to produce **smaller models** compared to LoRA at the same rank.

<details>
<summary>æ—¥æœ¬èª</summary>

### LoHa

LoHaã¯é‡ã¿ã®æ›´æ–°ã‚’2ã¤ã®ä½ãƒ©ãƒ³ã‚¯è¡Œåˆ—ã®Hadamardç©ï¼ˆè¦ç´ ã”ã¨ã®ç©ï¼‰ã§è¡¨ç¾ã—ã¾ã™:

```
Î”W = (W1a Ã— W1b) âŠ™ (W2a Ã— W2b)
```

ã“ã“ã§ `W1a`, `W1b`, `W2a`, `W2b` ã¯ãƒ©ãƒ³ã‚¯ `network_dim` ã®ä½ãƒ©ãƒ³ã‚¯è¡Œåˆ—ã§ã™ã€‚LoHaã¯åŒã˜ãƒ©ãƒ³ã‚¯ã®LoRAã¨æ¯”è¼ƒã—ã¦å­¦ç¿’å¯èƒ½ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ãŒ **ç´„2å€** ã«ãªã‚Šã¾ã™ãŒã€è¦ç´ ã”ã¨ã®ç©ã«ã‚ˆã‚Šã€ã‚ˆã‚Šè¤‡é›‘ãªé‡ã¿æ§‹é€ ã‚’æ‰ãˆã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚

### LoKr

LoKrã¯Kroneckerç©ã‚’ä½¿ã£ã¦é‡ã¿ã®æ›´æ–°ã‚’è¡¨ç¾ã—ã¾ã™:

```
Î”W = W1 âŠ— W2    ï¼ˆä½ãƒ©ãƒ³ã‚¯ãƒ¢ãƒ¼ãƒ‰ã§ã¯ W2 = W2a Ã— W2bï¼‰
```

å…ƒã®é‡ã¿ã®æ¬¡å…ƒãŒå› æ•°åˆ†è§£ã•ã‚Œã¾ã™ï¼ˆä¾‹: 512Ã—512ã®é‡ã¿ãŒã€W1ãŒ16Ã—16ã€W2ãŒ32Ã—32ã«åˆ†å‰²ã•ã‚Œã¾ã™ï¼‰ã€‚W1ã¯å¸¸ã«ãƒ•ãƒ«è¡Œåˆ—ï¼ˆå°ã•ã„ï¼‰ã§ã€W2ã¯ãƒ©ãƒ³ã‚¯è¨­å®šã«å¿œã˜ã¦ä½ãƒ©ãƒ³ã‚¯åˆ†è§£ã¾ãŸã¯ãƒ•ãƒ«è¡Œåˆ—ã«ãªã‚Šã¾ã™ã€‚LoKrã¯åŒã˜ãƒ©ãƒ³ã‚¯ã®LoRAã¨æ¯”è¼ƒã—ã¦ **ã‚ˆã‚Šå°ã•ã„ãƒ¢ãƒ‡ãƒ«** ã‚’ç”Ÿæˆã™ã‚‹å‚¾å‘ãŒã‚ã‚Šã¾ã™ã€‚

</details>

## Inference / æ¨è«–

Trained LoHa/LoKr weights are saved in safetensors format, just like LoRA. The inference method depends on the architecture.

<details>
<summary>æ—¥æœ¬èª</summary>

å­¦ç¿’æ¸ˆã¿ã®LoHa/LoKrã®é‡ã¿ã¯ã€LoRAã¨åŒæ§˜ã«safetensorså½¢å¼ã§ä¿å­˜ã•ã‚Œã¾ã™ã€‚æ¨è«–æ–¹æ³•ã¯ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã«ã‚ˆã£ã¦ç•°ãªã‚Šã¾ã™ã€‚

</details>

### Architectures with built-in support / ãƒã‚¤ãƒ†ã‚£ãƒ–ã‚µãƒãƒ¼ãƒˆã®ã‚ã‚‹ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£

The following architectures automatically detect and load LoHa/LoKr weights without any additional options:

- Wan 2.1/2.2
- FramePack
- HunyuanVideo 1.5
- FLUX.2
- Qwen-Image series
- Z-Image

Use `--lora_weight` as usual:

```bash
python src/musubi_tuner/wan_generate_video.py ... --lora_weight path/to/loha_or_lokr.safetensors
```

<details>
<summary>æ—¥æœ¬èª</summary>

ä»¥ä¸‹ã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã§ã¯ã€LoHa/LoKrã®é‡ã¿ã‚’è¿½åŠ ã‚ªãƒ—ã‚·ãƒ§ãƒ³ãªã—ã§è‡ªå‹•æ¤œå‡ºã—ã¦èª­ã¿è¾¼ã¿ã¾ã™:

- Wan 2.1/2.2
- FramePack
- HunyuanVideo 1.5
- FLUX.2
- Qwen-Imageç³»
- Z-Image

é€šå¸¸é€šã‚Š `--lora_weight` ã‚’ä½¿ç”¨ã—ã¾ã™:

```bash
python src/musubi_tuner/wan_generate_video.py ... --lora_weight path/to/loha_or_lokr.safetensors
```

</details>

### HunyuanVideo / FLUX.1 Kontext

For HunyuanVideo and FLUX.1 Kontext, the `--lycoris` option is required, and the [LyCORIS library](https://github.com/KohakuBlueleaf/LyCORIS) must be installed:

```bash
pip install lycoris-lora

python src/musubi_tuner/hv_generate_video.py ... --lora_weight path/to/loha_or_lokr.safetensors --lycoris
```

<details>
<summary>æ—¥æœ¬èª</summary>

HunyuanVideoã¨FLUX.1 Kontextã§ã¯ã€`--lycoris` ã‚ªãƒ—ã‚·ãƒ§ãƒ³ãŒå¿…è¦ã§ã€[LyCORIS ãƒ©ã‚¤ãƒ–ãƒ©ãƒª](https://github.com/KohakuBlueleaf/LyCORIS)ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ãŒå¿…è¦ã§ã™:

```bash
pip install lycoris-lora

python src/musubi_tuner/hv_generate_video.py ... --lora_weight path/to/loha_or_lokr.safetensors --lycoris
```

</details>

## Limitations / åˆ¶é™äº‹é …

### LoRA+ is not supported / LoRA+ã¯éå¯¾å¿œ

LoRA+ (`loraplus_lr_ratio` in `--network_args`) is **not supported** with LoHa/LoKr. LoRA+ works by applying different learning rates to the LoRA-A and LoRA-B matrices, which is specific to the standard LoRA architecture. LoHa and LoKr have different parameter structures and this optimization does not apply.

<details>
<summary>æ—¥æœ¬èª</summary>

LoRA+ï¼ˆ`--network_args` ã® `loraplus_lr_ratio`ï¼‰ã¯LoHa/LoKrã§ã¯ **éå¯¾å¿œ** ã§ã™ã€‚LoRA+ã¯LoRA-Aã¨LoRA-Bã®è¡Œåˆ—ã«ç•°ãªã‚‹å­¦ç¿’ç‡ã‚’é©ç”¨ã™ã‚‹æ‰‹æ³•ã§ã‚ã‚Šã€æ¨™æº–çš„ãªLoRAã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã«å›ºæœ‰ã®ã‚‚ã®ã§ã™ã€‚LoHaã¨LoKrã¯ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ§‹é€ ãŒç•°ãªã‚‹ãŸã‚ã€ã“ã®æœ€é©åŒ–ã¯é©ç”¨ã•ã‚Œã¾ã›ã‚“ã€‚

</details>

### Merging to base model / ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã¸ã®ãƒãƒ¼ã‚¸

`merge_lora.py` currently supports standard LoRA only. LoHa/LoKr weights cannot be merged into the base model using this script.

For architectures with built-in LoHa/LoKr support (listed above), merging is performed automatically during model loading at inference time, so this limitation only affects offline merging workflows.

<details>
<summary>æ—¥æœ¬èª</summary>

`merge_lora.py` ã¯ç¾åœ¨ã€æ¨™æº–LoRAã®ã¿ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã¾ã™ã€‚ã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã§ã¯LoHa/LoKrã®é‡ã¿ã‚’ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã«ãƒãƒ¼ã‚¸ã™ã‚‹ã“ã¨ã¯ã§ãã¾ã›ã‚“ã€‚

LoHa/LoKrã®ãƒã‚¤ãƒ†ã‚£ãƒ–ã‚µãƒãƒ¼ãƒˆãŒã‚ã‚‹ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ï¼ˆä¸Šè¨˜ï¼‰ã§ã¯ã€æ¨è«–æ™‚ã®ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿æ™‚ã«ãƒãƒ¼ã‚¸ãŒè‡ªå‹•çš„ã«è¡Œã‚ã‚Œã‚‹ãŸã‚ã€ã“ã®åˆ¶é™ã¯ã‚ªãƒ•ãƒ©ã‚¤ãƒ³ãƒãƒ¼ã‚¸ã®ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã«ã®ã¿å½±éŸ¿ã—ã¾ã™ã€‚

</details>

### Format conversion / ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›

`convert_lora.py` is extended to also support format conversion of LoHa/LoKr weights between Musubi Tuner format and Diffusers format for ComfyUI.

<details>
<summary>æ—¥æœ¬èª</summary>

`convert_lora.py` ã¯ã€LoRAã«åŠ ãˆã¦ã€LoHa/LoKrã®é‡ã¿ã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ï¼ˆMusubi Tunerå½¢å¼ã¨Diffuserså½¢å¼é–“ã®å¤‰æ›ï¼‰ã«ã¤ã„ã¦ã‚‚ã‚µãƒãƒ¼ãƒˆã™ã‚‹ã‚ˆã†ã€æ‹¡å¼µã•ã‚Œã¦ã„ã¾ã™ã€‚

</details>
